{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyP9hCND+byHnYMfPJzxmrSr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# Google Drive 마운트 확인\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# 데이터셋 폴더로 이동\n","import os\n","os.chdir('/content/drive/MyDrive/2025_zolzak/')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zB3-KMXHIylX","executionInfo":{"status":"ok","timestamp":1762243777258,"user_tz":-540,"elapsed":113906,"user":{"displayName":"김현규","userId":"12850225710756885616"}},"outputId":"50901536-0096-44c0-aa41-eef9687b29d8"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!pip install ultralytics"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"purfmQo5Iyb1","executionInfo":{"status":"ok","timestamp":1762243808282,"user_tz":-540,"elapsed":6294,"user":{"displayName":"김현규","userId":"12850225710756885616"}},"outputId":"45dace46-0e02-4cff-9ba0-6f93155bb37d"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ultralytics\n","  Downloading ultralytics-8.3.224-py3-none-any.whl.metadata (37 kB)\n","Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.2)\n","Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\n","Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (4.12.0.88)\n","Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.3.0)\n","Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.3)\n","Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.3)\n","Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.8.0+cu126)\n","Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.23.0+cu126)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n","Requirement already satisfied: polars in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.25.2)\n","Collecting ultralytics-thop>=2.0.18 (from ultralytics)\n","  Downloading ultralytics_thop-2.0.18-py3-none-any.whl.metadata (14 kB)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.60.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.5)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2025.10.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.20.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.4.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.3)\n","Downloading ultralytics-8.3.224-py3-none-any.whl (1.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ultralytics_thop-2.0.18-py3-none-any.whl (28 kB)\n","Installing collected packages: ultralytics-thop, ultralytics\n","Successfully installed ultralytics-8.3.224 ultralytics-thop-2.0.18\n"]}]},{"cell_type":"code","source":["import torch\n","import pickle\n","import numpy as np\n","from tqdm import tqdm\n","from pathlib import Path\n","from torch.utils.data import DataLoader, Dataset\n","import torchvision.transforms as T\n","from ultralytics import YOLO\n","from ultralytics.data.augment import LetterBox\n","from ultralytics.utils.instance import Instances\n","from PIL import Image\n","import time\n","import cv2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"61_oD6YMIxjJ","executionInfo":{"status":"ok","timestamp":1762243818945,"user_tz":-540,"elapsed":10669,"user":{"displayName":"김현규","userId":"12850225710756885616"}},"outputId":"46a72b35-476c-484c-8210-48c532ed2c47"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Creating new Ultralytics Settings v0.0.6 file ✅ \n","View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n","Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"]}]},{"cell_type":"code","source":["# ============= CUDA CLAHE 초기화 =============\n","try:\n","    clahe_cuda = cv2.cuda.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n","    USE_CUDA_CLAHE = True\n","    print(\"✓ CUDA CLAHE initialized successfully\")\n","except Exception as e:\n","    print(f\"✗ CUDA CLAHE initialization failed: {e}\")\n","    print(\"  Falling back to CPU CLAHE\")\n","    clahe_cpu = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n","    USE_CUDA_CLAHE = False"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zR6LqsEDJIp4","executionInfo":{"status":"ok","timestamp":1762243820345,"user_tz":-540,"elapsed":17,"user":{"displayName":"김현규","userId":"12850225710756885616"}},"outputId":"31d6c3aa-d00b-47c2-ac53-82ed641a8504"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["✗ CUDA CLAHE initialization failed: module 'cv2.cuda' has no attribute 'createCLAHE'\n","  Falling back to CPU CLAHE\n"]}]},{"cell_type":"code","source":["# ============= 설정 =============\n","with open(\"id_stats.pkl\", \"rb\") as f:\n","    stats = pickle.load(f)\n","\n","mu_np, cov_inv_np, threshold = stats[\"mu\"], stats[\"cov_inv\"], stats[\"threshold\"]\n","\n","device = torch.device(\"cuda\")\n","mu = torch.from_numpy(mu_np).float().to(device)\n","cov_inv = torch.from_numpy(cov_inv_np).float().to(device)\n","\n","eval_val_img_dir = \"dataset_prepared_05/eval/val/images/\"\n","eval_val_lbl_dir = \"dataset_prepared_05/eval/val/labels/\"\n","\n","model = YOLO(\"runs/train/dp05_yolov105/weights/best.pt\")\n","model.to(device)\n","model.eval()\n","\n","neck_layer = model.model.model[22]"],"metadata":{"id":"k-bo9gpmJLGQ","executionInfo":{"status":"ok","timestamp":1762243826949,"user_tz":-540,"elapsed":5600,"user":{"displayName":"김현규","userId":"12850225710756885616"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# ============= CLAHE 적용 함수 =============\n","def apply_clahe_cuda(img_rgb):\n","    if USE_CUDA_CLAHE:\n","        img_ycrcb = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2YCrCb)\n","        y_channel = img_ycrcb[:, :, 0]\n","        gpu_y = cv2.cuda_GpuMat()\n","        gpu_y.upload(y_channel)\n","        gpu_y_clahe = clahe_cuda.apply(gpu_y)\n","        y_clahe = gpu_y_clahe.download()\n","        img_ycrcb[:, :, 0] = y_clahe\n","        clahe_rgb = cv2.cvtColor(img_ycrcb, cv2.COLOR_YCrCb2RGB)\n","    else:\n","        img_ycrcb = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2YCrCb)\n","        img_ycrcb[:, :, 0] = clahe_cpu.apply(img_ycrcb[:, :, 0])\n","        clahe_rgb = cv2.cvtColor(img_ycrcb, cv2.COLOR_YCrCb2RGB)\n","    return clahe_rgb"],"metadata":{"id":"k-SdX2RkJSRo","executionInfo":{"status":"ok","timestamp":1762243882047,"user_tz":-540,"elapsed":59,"user":{"displayName":"김현규","userId":"12850225710756885616"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# ============= 배치 마할라노비스 거리 계산 =============\n","def batch_mahalanobis_distance(X, mu, cov_inv):\n","    diff = X - mu.unsqueeze(0)\n","    left = torch.matmul(diff, cov_inv)\n","    distances = torch.sqrt(torch.sum(left * diff, dim=1))\n","    return distances\n","\n","class FeatureExtractor:\n","    def __init__(self, model, layer):\n","        self.features = None\n","        self.hook_handle = layer.register_forward_hook(self.hook_fn)\n","\n","    def hook_fn(self, module, input, output):\n","        self.features = torch.mean(output, dim=(2, 3))\n","\n","    def remove(self):\n","        self.hook_handle.remove()"],"metadata":{"id":"nTRuBNjtJTqu","executionInfo":{"status":"ok","timestamp":1762243883247,"user_tz":-540,"elapsed":13,"user":{"displayName":"김현규","userId":"12850225710756885616"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# ============= OOD 경고 오버레이 생성 =============\n","# ============= 모델 imgsz와 원본 크기에 맞도록 조정 필요 =============\n","def create_ood_warning_overlay(img_shape, intensity=0.3, border_width=None):\n","    \"\"\"\n","    네비게이션 스타일의 OOD 경고 오버레이 생성 (자동 크기 조정)\n","    \"\"\"\n","    H, W = img_shape[:2]\n","\n","    # 이미지 크기에 비례하는 테두리 두께\n","    if border_width is None:\n","        border_width = max(20, int(min(H, W) * 0.02))  # 이미지 크기의 2%\n","\n","    overlay = np.zeros((H, W, 3), dtype=np.uint8)\n","\n","    red_color = int(255 * intensity)\n","    overlay[:border_width, :] = [0, 0, red_color]\n","    overlay[-border_width:, :] = [0, 0, red_color]\n","    overlay[:, :border_width] = [0, 0, red_color]\n","    overlay[:, -border_width:] = [0, 0, red_color]\n","\n","    corner_size = border_width * 2\n","    overlay[:corner_size, :corner_size] = [0, 0, 255]\n","    overlay[:corner_size, -corner_size:] = [0, 0, 255]\n","    overlay[-corner_size:, :corner_size] = [0, 0, 255]\n","    overlay[-corner_size:, -corner_size:] = [0, 0, 255]\n","\n","    return overlay\n","\n","def draw_ood_warning(img, ood_detected, alpha=0.5):\n","    \"\"\"\n","    이미지에 OOD 경고 표시 (원본 크기 자동 조정)\n","    \"\"\"\n","    if not ood_detected:\n","        return img\n","\n","    overlay = create_ood_warning_overlay(img.shape, intensity=0.6)\n","    overlay = cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR)\n","    img_with_warning = cv2.addWeighted(img, 1, overlay, alpha, 0)\n","\n","    # 텍스트 크기도 이미지에 비례하게\n","    H, W = img.shape[:2]\n","    font_scale = max(1.0, min(H, W) / 1000)  # 동적 폰트 크기\n","    thickness = max(2, int(font_scale * 2))\n","\n","    font = cv2.FONT_HERSHEY_SIMPLEX\n","    text = \"⚠ OOD DETECTED!\"\n","\n","    (text_w, text_h), _ = cv2.getTextSize(text, font, font_scale, thickness)\n","\n","    x = (W - text_w) // 2\n","    y = int(H * 0.08)  # 상단 8% 위치\n","\n","    # 텍스트 배경\n","    padding = 15\n","    cv2.rectangle(img_with_warning,\n","                  (x - padding, y - text_h - padding),\n","                  (x + text_w + padding, y + padding),\n","                  (0, 0, 0), -1)\n","\n","    # 텍스트\n","    cv2.putText(img_with_warning, text, (x, y),\n","                font, font_scale, (0, 0, 255), thickness)\n","\n","    return img_with_warning\n","\n","# def create_ood_warning_overlay(img_shape, intensity=0.3, border_width=30):\n","#     \"\"\"\n","#     네비게이션 스타일의 OOD 경고 오버레이 생성\n","#     Args:\n","#         img_shape: (H, W, C)\n","#         intensity: 빨간색 강도 (0~1)\n","#         border_width: 테두리 두께 (픽셀)\n","#     Returns:\n","#         overlay: (H, W, 3) uint8 배열\n","#     \"\"\"\n","#     H, W = img_shape[:2]\n","#     overlay = np.zeros((H, W, 3), dtype=np.uint8)\n","\n","#     # 빨간색 테두리 (상하좌우)\n","#     red_color = int(255 * intensity)\n","#     overlay[:border_width, :] = [red_color, 0, 0]  # 상단\n","#     overlay[-border_width:, :] = [red_color, 0, 0]  # 하단\n","#     overlay[:, :border_width] = [red_color, 0, 0]  # 좌측\n","#     overlay[:, -border_width:] = [red_color, 0, 0]  # 우측\n","\n","#     # 코너 강조 (선택적)\n","#     corner_size = border_width * 2\n","#     overlay[:corner_size, :corner_size] = [255, 0, 0]  # 좌상\n","#     overlay[:corner_size, -corner_size:] = [255, 0, 0]  # 우상\n","#     overlay[-corner_size:, :corner_size] = [255, 0, 0]  # 좌하\n","#     overlay[-corner_size:, -corner_size:] = [255, 0, 0]  # 우하\n","\n","#     return overlay\n","\n","# def draw_ood_warning(img, ood_detected, alpha=0.5):\n","#     \"\"\"\n","#     이미지에 OOD 경고 표시\n","#     Args:\n","#         img: (H, W, 3) BGR 이미지\n","#         ood_detected: bool, OOD 검출 여부\n","#         alpha: 오버레이 투명도\n","#     Returns:\n","#         img_with_warning: (H, W, 3) BGR 이미지\n","#     \"\"\"\n","#     if not ood_detected:\n","#         return img\n","\n","#     overlay = create_ood_warning_overlay(img.shape, intensity=0.6, border_width=20)\n","#     # RGB -> BGR 변환\n","#     overlay = cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR)\n","\n","#     # 블렌딩\n","#     img_with_warning = cv2.addWeighted(img, 1, overlay, alpha, 0)\n","\n","#     # \"OOD DETECTED!\" 텍스트 추가\n","#     font = cv2.FONT_HERSHEY_SIMPLEX\n","#     text = \"⚠ OOD DETECTED!\"\n","#     font_scale = 1.2\n","#     thickness = 3\n","\n","#     # 텍스트 크기 계산\n","#     (text_w, text_h), _ = cv2.getTextSize(text, font, font_scale, thickness)\n","\n","#     # 중앙 상단에 배치\n","#     x = (img.shape[1] - text_w) // 2\n","#     y = 50\n","\n","#     # 텍스트 배경 (검은색 반투명)\n","#     cv2.rectangle(img_with_warning,\n","#                   (x-10, y-text_h-10),\n","#                   (x+text_w+10, y+10),\n","#                   (0, 0, 0), -1)\n","\n","#     # 텍스트 (빨간색)\n","#     cv2.putText(img_with_warning, text, (x, y),\n","#                 font, font_scale, (0, 0, 255), thickness)\n","\n","#     return img_with_warning\n","\n","# def draw_bboxes(img, results, conf_threshold=0.001):\n","#     \"\"\"\n","#     YOLO 결과에서 ID 클래스 bbox 그리기\n","#     Args:\n","#         img: (H, W, 3) BGR 이미지\n","#         results: YOLO results 객체\n","#         conf_threshold: confidence threshold\n","#     Returns:\n","#         img_with_boxes: (H, W, 3) BGR 이미지\n","#     \"\"\"\n","#     img_with_boxes = img.copy()\n","\n","#     if results and len(results) > 0:\n","#         result = results[0]  # 첫 번째 이미지 결과\n","\n","#         if result.boxes is not None and len(result.boxes) > 0:\n","#             boxes = result.boxes.cpu().numpy()\n","\n","#             for box in boxes:\n","#                 conf = box.conf[0]\n","#                 if conf < conf_threshold:\n","#                     continue\n","\n","#                 cls_id = int(box.cls[0])\n","#                 x1, y1, x2, y2 = box.xyxy[0].astype(int)\n","\n","#                 # bbox 그리기 (녹색)\n","#                 cv2.rectangle(img_with_boxes, (x1, y1), (x2, y2), (0, 255, 0), 2)\n","\n","#                 # 라벨 배경\n","#                 label = f\"Class {cls_id}: {conf:.2f}\"\n","#                 (label_w, label_h), _ = cv2.getTextSize(\n","#                     label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1\n","#                 )\n","#                 cv2.rectangle(img_with_boxes,\n","#                              (x1, y1-label_h-10),\n","#                              (x1+label_w, y1),\n","#                              (0, 255, 0), -1)\n","\n","#                 # 라벨 텍스트\n","#                 cv2.putText(img_with_boxes, label,\n","#                            (x1, y1-5),\n","#                            cv2.FONT_HERSHEY_SIMPLEX,\n","#                            0.5, (0, 0, 0), 1)\n","\n","#     return img_with_boxes\n","\n","def draw_bboxes(img, results, original_shape, model_input_shape=(544, 960), conf_threshold=0.001):\n","    \"\"\"\n","    YOLO 결과에서 ID 클래스 bbox 그리기 (좌표 스케일 보정 포함)\n","    Args:\n","        img: (H, W, 3) BGR 이미지 (원본 크기)\n","        results: YOLO results 객체\n","        original_shape: (H, W) 원본 이미지 크기\n","        model_input_shape: (H, W) 모델 입력 크기 (544, 960)\n","        conf_threshold: confidence threshold\n","    Returns:\n","        img_with_boxes: (H, W, 3) BGR 이미지\n","    \"\"\"\n","    img_with_boxes = img.copy()\n","    orig_h, orig_w = original_shape\n","    model_h, model_w = model_input_shape\n","\n","    # LetterBox 적용 시 스케일 계산\n","    # LetterBox는 aspect ratio를 유지하면서 리사이즈\n","    scale = min(model_h / orig_h, model_w / orig_w)\n","\n","    # 실제 리사이즈된 크기\n","    new_h = int(orig_h * scale)\n","    new_w = int(orig_w * scale)\n","\n","    # 패딩 계산\n","    pad_h = (model_h - new_h) / 2\n","    pad_w = (model_w - new_w) / 2\n","\n","    if results and len(results) > 0:\n","        result = results[0]  # 첫 번째 이미지 결과\n","\n","        if result.boxes is not None and len(result.boxes) > 0:\n","            boxes = result.boxes.cpu().numpy()\n","\n","            for box in boxes:\n","                conf = box.conf[0]\n","                if conf < conf_threshold:\n","                    continue\n","\n","                cls_id = int(box.cls[0])\n","\n","                # 모델 출력 좌표 (960x544 기준)\n","                x1_model, y1_model, x2_model, y2_model = box.xyxy[0]\n","\n","                # 패딩 제거\n","                x1_unpad = x1_model - pad_w\n","                y1_unpad = y1_model - pad_h\n","                x2_unpad = x2_model - pad_w\n","                y2_unpad = y2_model - pad_h\n","\n","                # 원본 크기로 스케일 복원\n","                x1_orig = int(x1_unpad / scale)\n","                y1_orig = int(y1_unpad / scale)\n","                x2_orig = int(x2_unpad / scale)\n","                y2_orig = int(y2_unpad / scale)\n","\n","                # 이미지 경계 내로 클리핑\n","                x1_orig = max(0, min(x1_orig, orig_w))\n","                y1_orig = max(0, min(y1_orig, orig_h))\n","                x2_orig = max(0, min(x2_orig, orig_w))\n","                y2_orig = max(0, min(y2_orig, orig_h))\n","\n","                # bbox 그리기 (녹색)\n","                cv2.rectangle(img_with_boxes,\n","                             (x1_orig, y1_orig), (x2_orig, y2_orig),\n","                             (0, 255, 0), 3)  # 두께 2->3 (큰 이미지용)\n","\n","                # 라벨 배경\n","                label = f\"Class {cls_id}: {conf:.2f}\"\n","                font_scale = 0.8  # 0.5 -> 0.8 (큰 이미지용)\n","                thickness = 2\n","                (label_w, label_h), _ = cv2.getTextSize(\n","                    label, cv2.FONT_HERSHEY_SIMPLEX, font_scale, thickness\n","                )\n","\n","                # 라벨이 이미지 밖으로 나가지 않도록\n","                label_y = max(label_h + 10, y1_orig)\n","\n","                cv2.rectangle(img_with_boxes,\n","                             (x1_orig, label_y - label_h - 10),\n","                             (x1_orig + label_w + 10, label_y),\n","                             (0, 255, 0), -1)\n","\n","                # 라벨 텍스트\n","                cv2.putText(img_with_boxes, label,\n","                           (x1_orig + 5, label_y - 5),\n","                           cv2.FONT_HERSHEY_SIMPLEX,\n","                           font_scale, (0, 0, 0), thickness)\n","\n","    return img_with_boxes"],"metadata":{"id":"M9e4XjgRJVLe","executionInfo":{"status":"ok","timestamp":1762244664600,"user_tz":-540,"elapsed":21,"user":{"displayName":"김현규","userId":"12850225710756885616"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# ============= 데이터셋 =============\n","def load_gt_labels(label_dir):\n","    gt_map = {}\n","    for txt_file in Path(label_dir).glob(\"*.txt\"):\n","        with open(txt_file, \"r\", encoding=\"utf-8\") as f:\n","            lines = f.readlines()\n","            fname = txt_file.stem\n","            gt_map[fname] = [int(line.split()[0]) for line in lines if line.strip()]\n","    return gt_map\n","\n","class OODDatasetWithCLAHE(Dataset):\n","    def __init__(self, img_dir, label_dir, gt_labels, imgsz=960, use_clahe=True):\n","        self.img_dir = Path(img_dir)\n","        self.label_dir = Path(label_dir) if label_dir else None\n","        self.gt_labels = gt_labels\n","        self.imgsz = imgsz\n","        self.use_clahe = use_clahe\n","\n","        # 모든 이미지 (OOD만이 아닌 전체)\n","        self.img_paths = []\n","        for p in sorted(self.img_dir.iterdir()):\n","            if p.suffix.lower() in ['.jpg', '.jpeg', '.png']:\n","                self.img_paths.append(str(p))\n","\n","        print(f\"Found {len(self.img_paths)} images\")\n","        print(f\"CLAHE preprocessing: {'Enabled' if use_clahe else 'Disabled'}\")\n","\n","        self.letterbox = LetterBox(new_shape=(544, 960), auto=False)\n","\n","    def __len__(self):\n","        return len(self.img_paths)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.img_paths[idx]\n","        stem = Path(img_path).stem\n","\n","        # 원본 이미지 로드 (시각화용)\n","        original_img = cv2.imread(img_path)\n","\n","        img = Image.open(img_path).convert(\"RGB\")\n","        img_array = np.array(img)\n","\n","        # CLAHE 적용\n","        if self.use_clahe:\n","            img_array = apply_clahe_cuda(img_array)\n","\n","        # 라벨 로드\n","        if self.label_dir and (self.label_dir / f\"{stem}.txt\").exists():\n","            with open(self.label_dir / f\"{stem}.txt\") as f:\n","                lines = [list(map(float, line.strip().split()))\n","                        for line in f if line.strip()]\n","            if lines:\n","                cls, bboxes = zip(*[(int(l[0]), l[1:]) for l in lines])\n","                bboxes = np.array(bboxes, dtype=np.float32)\n","                cls = np.array(cls)\n","            else:\n","                bboxes = np.zeros((0, 4), dtype=np.float32)\n","                cls = np.array([])\n","        else:\n","            bboxes = np.zeros((0, 4), dtype=np.float32)\n","            cls = np.array([])\n","\n","        inst = Instances(\n","            bboxes=bboxes,\n","            segments=np.zeros((len(bboxes), 0, 2), dtype=np.float32),\n","            bbox_format=\"xywh\"\n","        )\n","\n","        sample = {\n","            \"img\": img_array,\n","            \"cls\": cls,\n","            \"instances\": inst,\n","            \"ori_shape\": img_array.shape[:2],\n","            \"resized_shape\": img_array.shape[:2],\n","            \"path\": str(img_path),\n","            \"batch_idx\": 0,\n","            \"original_img\": original_img  # 시각화용 원본 추가\n","        }\n","\n","        sample = self.letterbox(sample)\n","        img_tensor = torch.from_numpy(sample[\"img\"]).permute(2, 0, 1).contiguous()\n","        sample[\"img\"] = img_tensor\n","\n","        return sample\n","\n","def custom_collate_fn(batch):\n","    collated = {}\n","    for key in batch[0].keys():\n","        if key == \"instances\":\n","            collated[key] = [item[key] for item in batch]\n","        elif key in [\"img\"]:\n","            collated[key] = torch.stack([item[key] for item in batch])\n","        elif key in [\"original_img\"]:\n","            collated[key] = [item[key] for item in batch]\n","        else:\n","            collated[key] = [item[key] for item in batch]\n","    return collated"],"metadata":{"id":"T17IIYLgJYrf","executionInfo":{"status":"ok","timestamp":1762244668447,"user_tz":-540,"elapsed":27,"user":{"displayName":"김현규","userId":"12850225710756885616"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["# # ============= 통합 추론 및 시각화 파이프라인 =============\n","# def inference_with_visualization(model, dataloader, mu, cov_inv, threshold,\n","#                                  device, save_dir=\"output_visualizations\"):\n","#     \"\"\"\n","#     OOD 검출 + ID bbox 그리기 + 시각화 통합 파이프라인\n","#     \"\"\"\n","#     save_dir = Path(save_dir)\n","#     save_dir.mkdir(exist_ok=True, parents=True)\n","\n","#     feature_extractor = FeatureExtractor(model, neck_layer)\n","\n","#     all_scores = []\n","#     all_results = []  # YOLO detection 결과 저장\n","#     all_original_imgs = []\n","#     all_paths = []\n","\n","#     model.eval()\n","\n","#     with torch.no_grad():\n","#         for batch_idx, batch in enumerate(tqdm(dataloader, desc=\"Processing\")):\n","#             imgs = batch[\"img\"].to(device, non_blocking=True).float() / 255.0\n","#             original_imgs = batch[\"original_img\"]\n","#             paths = batch[\"path\"]\n","\n","#             # YOLO 추론 (bbox 검출)\n","#             results = model(imgs, conf=0.001, verbose=False)\n","\n","#             # Feature 추출 (OOD 검출용)\n","#             if feature_extractor.features is not None:\n","#                 feats = feature_extractor.features\n","#             else:\n","#                 feat_map = model.model.model[:23](imgs)\n","#                 feats = torch.mean(feat_map, dim=(2, 3))\n","\n","#             # 마할라노비스 거리 계산\n","#             distances = batch_mahalanobis_distance(feats, mu, cov_inv)\n","\n","#             all_scores.append(distances)\n","#             all_results.extend(results)\n","#             all_original_imgs.extend(original_imgs)\n","#             all_paths.extend(paths)\n","\n","#     feature_extractor.remove()\n","\n","#     # 모든 스코어 결합\n","#     all_scores = torch.cat(all_scores, dim=0).cpu().numpy()\n","#     ood_predictions = all_scores > threshold\n","\n","#     # 시각화\n","#     print(\"\\nGenerating visualizations...\")\n","#     for idx, (img, score, is_ood, result, path) in enumerate(\n","#         tqdm(zip(all_original_imgs, all_scores, ood_predictions, all_results, all_paths),\n","#              total=len(all_original_imgs), desc=\"Saving\")\n","#     ):\n","#         # 1. ID 클래스 bbox 그리기\n","#         img_with_boxes = draw_bboxes(img, [result], conf_threshold=0.001)\n","\n","#         # 2. OOD 경고 표시\n","#         img_final = draw_ood_warning(img_with_boxes, is_ood, alpha=0.5)\n","\n","#         # 3. 스코어 정보 표시 (우하단)\n","#         info_text = f\"Score: {score:.2f} | Threshold: {threshold:.2f}\"\n","#         status_text = \"OOD\" if is_ood else \"ID\"\n","#         status_color = (0, 0, 255) if is_ood else (0, 255, 0)\n","\n","#         cv2.putText(img_final, info_text, (10, img_final.shape[0]-40),\n","#                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n","#         cv2.putText(img_final, f\"Status: {status_text}\", (10, img_final.shape[0]-10),\n","#                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, status_color, 2)\n","\n","#         # 4. 저장\n","#         filename = Path(path).stem\n","#         save_path = save_dir / f\"{filename}_result.jpg\"\n","#         cv2.imwrite(str(save_path), img_final)\n","\n","#     return all_scores, ood_predictions"],"metadata":{"id":"k22XhO_zJbaC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def inference_with_visualization(model, dataloader, mu, cov_inv, threshold,\n","                                 device, save_dir=\"output_visualizations\",\n","                                 model_input_shape=(544, 960)):\n","    \"\"\"\n","    OOD 검출 + ID bbox 그리기 + 시각화 통합 파이프라인\n","    \"\"\"\n","    save_dir = Path(save_dir)\n","    save_dir.mkdir(exist_ok=True, parents=True)\n","\n","    feature_extractor = FeatureExtractor(model, neck_layer)\n","\n","    all_scores = []\n","    all_results = []\n","    all_original_imgs = []\n","    all_paths = []\n","\n","    model.eval()\n","\n","    with torch.no_grad():\n","        for batch_idx, batch in enumerate(tqdm(dataloader, desc=\"Processing\")):\n","            imgs = batch[\"img\"].to(device, non_blocking=True).float() / 255.0\n","            original_imgs = batch[\"original_img\"]\n","            paths = batch[\"path\"]\n","\n","            # YOLO 추론\n","            results = model(imgs, conf=0.001, verbose=False)\n","\n","            # Feature 추출\n","            if feature_extractor.features is not None:\n","                feats = feature_extractor.features\n","            else:\n","                feat_map = model.model.model[:23](imgs)\n","                feats = torch.mean(feat_map, dim=(2, 3))\n","\n","            distances = batch_mahalanobis_distance(feats, mu, cov_inv)\n","\n","            all_scores.append(distances)\n","            all_results.extend(results)\n","            all_original_imgs.extend(original_imgs)\n","            all_paths.extend(paths)\n","\n","    feature_extractor.remove()\n","\n","    all_scores = torch.cat(all_scores, dim=0).cpu().numpy()\n","    ood_predictions = all_scores > threshold\n","\n","    # 시각화\n","    print(\"\\nGenerating visualizations...\")\n","    for idx, (img, score, is_ood, result, path) in enumerate(\n","        tqdm(zip(all_original_imgs, all_scores, ood_predictions, all_results, all_paths),\n","             total=len(all_original_imgs), desc=\"Saving\")\n","    ):\n","        orig_h, orig_w = img.shape[:2]\n","\n","        # 1. ID 클래스 bbox 그리기 (스케일 보정 포함)\n","        img_with_boxes = draw_bboxes(\n","            img, [result],\n","            original_shape=(orig_h, orig_w),\n","            model_input_shape=model_input_shape,\n","            conf_threshold=0.001\n","        )\n","\n","        # 2. OOD 경고 표시\n","        img_final = draw_ood_warning(img_with_boxes, is_ood, alpha=0.5)\n","\n","        # 3. 스코어 정보 (크기에 맞게 조정)\n","        font_scale = max(0.6, min(orig_h, orig_w) / 1500)\n","        thickness = max(2, int(font_scale * 3))\n","\n","        info_text = f\"Score: {score:.2f} | Threshold: {threshold:.2f}\"\n","        status_text = \"OOD\" if is_ood else \"ID\"\n","        status_color = (0, 0, 255) if is_ood else (0, 255, 0)\n","\n","        y_offset = orig_h - int(orig_h * 0.05)\n","        cv2.putText(img_final, info_text, (20, y_offset - 40),\n","                   cv2.FONT_HERSHEY_SIMPLEX, font_scale, (255, 255, 255), thickness)\n","        cv2.putText(img_final, f\"Status: {status_text}\", (20, y_offset),\n","                   cv2.FONT_HERSHEY_SIMPLEX, font_scale * 1.2, status_color, thickness)\n","\n","        # 4. 저장\n","        filename = Path(path).stem\n","        save_path = save_dir / f\"{filename}_result.jpg\"\n","        cv2.imwrite(str(save_path), img_final)\n","\n","    return all_scores, ood_predictions"],"metadata":{"id":"xtow996C59eG","executionInfo":{"status":"ok","timestamp":1762244673054,"user_tz":-540,"elapsed":6,"user":{"displayName":"김현규","userId":"12850225710756885616"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["# ============= 실행 =============\n","eval_val_gt_labels = load_gt_labels(eval_val_lbl_dir)\n","\n","fps_test_loader = DataLoader(\n","    OODDatasetWithCLAHE(\n","        eval_val_img_dir,\n","        eval_val_lbl_dir,\n","        eval_val_gt_labels,\n","        use_clahe=True\n","    ),\n","    batch_size=16,\n","    shuffle=False,\n","    collate_fn=custom_collate_fn,\n","    num_workers=4,\n","    pin_memory=True,\n","    persistent_workers=True\n",")\n","\n","num_imgs = len(fps_test_loader.dataset)\n","\n","# Warmup (첫 실행 오버헤드 제거)\n","print(\"Warming up...\")\n","_ = inference_with_visualization(\n","    model, fps_test_loader, mu, cov_inv, threshold,\n","    device, save_dir=\"output_visualizations_warmup\"\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c_yMhzIRJg_l","outputId":"7b866d15-7e2f-4302-a880-7eceebe6b3a6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 5247 images\n","CLAHE preprocessing: Enabled\n","Warming up...\n"]},{"output_type":"stream","name":"stderr","text":["Processing: 100%|██████████| 328/328 [02:39<00:00,  2.05it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Generating visualizations...\n"]},{"output_type":"stream","name":"stderr","text":["Saving:  32%|███▏      | 1696/5247 [07:31<40:48,  1.45it/s]"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d4-HcjuUImhk","executionInfo":{"status":"ok","timestamp":1762181903251,"user_tz":-540,"elapsed":226735,"user":{"displayName":"김현규","userId":"12850225710756885616"}},"outputId":"260325fa-a7fa-4e05-ff45-dc0ca2b27527"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","Actual measurement with visualization...\n"]},{"output_type":"stream","name":"stderr","text":["Processing: 100%|██████████| 328/328 [01:42<00:00,  3.21it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Generating visualizations...\n"]},{"output_type":"stream","name":"stderr","text":["Saving: 100%|██████████| 5247/5247 [02:04<00:00, 42.19it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","============================================================\n","Total Images: 5247\n","Inference + Visualization Time: 226.73 seconds\n","FPS: 23.14 frames per second\n","OOD Detected: 5235 / 5247\n","Output saved to: output_visualizations_final/\n","============================================================\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["# 실제 측정\n","print(\"\\n\\nActual measurement with visualization...\")\n","start = time.time()\n","pred_scores, pred_labels = inference_with_visualization(\n","    model, fps_test_loader, mu, cov_inv, threshold,\n","    device, save_dir=\"output_visualizations_final\"\n",")\n","end = time.time()\n","\n","inference_time = end - start\n","fps = num_imgs / inference_time\n","\n","print(f\"\\n{'='*60}\")\n","print(f\"Total Images: {num_imgs}\")\n","print(f\"Inference + Visualization Time: {inference_time:.2f} seconds\")\n","print(f\"FPS: {fps:.2f} frames per second\")\n","print(f\"OOD Detected: {pred_labels.sum()} / {num_imgs}\")\n","print(f\"Output saved to: output_visualizations_final/\")\n","print(f\"{'='*60}\")"]},{"cell_type":"code","source":["# CLAHE 비교 분석\n","\n","fps_test_loader = DataLoader(\n","    OODDatasetWithCLAHE(\n","        eval_val_img_dir,\n","        eval_val_lbl_dir,\n","        eval_val_gt_labels,\n","        use_clahe=False\n","    ),\n","    batch_size=16,\n","    shuffle=False,\n","    collate_fn=custom_collate_fn,\n","    num_workers=4,\n","    pin_memory=True,\n","    persistent_workers=True\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yn65b_tCUB-n","executionInfo":{"status":"ok","timestamp":1762183391148,"user_tz":-540,"elapsed":115,"user":{"displayName":"김현규","userId":"12850225710756885616"}},"outputId":"a04dc2b1-6924-432d-a74f-36587b054215"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 5247 images\n","CLAHE preprocessing: Disabled\n"]}]},{"cell_type":"code","source":["print(\"\\n\\nActual measurement with visualization...\")\n","start = time.time()\n","pred_scores, pred_labels = inference_with_visualization(\n","    model, fps_test_loader, mu, cov_inv, threshold,\n","    device, save_dir=\"output_visualizations_final_wo_clahe\"\n",")\n","end = time.time()\n","\n","inference_time = end - start\n","fps = num_imgs / inference_time\n","\n","print(f\"\\n{'='*60}\")\n","print(f\"Total Images: {num_imgs}\")\n","print(f\"Inference + Visualization Time: {inference_time:.2f} seconds\")\n","print(f\"FPS: {fps:.2f} frames per second\")\n","print(f\"OOD Detected: {pred_labels.sum()} / {num_imgs}\")\n","print(f\"Output saved to: output_visualizations_final_wo_clahe/\")\n","print(f\"{'='*60}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z_TzSR10R2GY","executionInfo":{"status":"ok","timestamp":1762183622187,"user_tz":-540,"elapsed":227286,"user":{"displayName":"김현규","userId":"12850225710756885616"}},"outputId":"7b6a4a28-31ff-488d-aa5f-c1333c6f4062"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","Actual measurement with visualization...\n"]},{"output_type":"stream","name":"stderr","text":["Processing: 100%|██████████| 328/328 [01:49<00:00,  3.01it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Generating visualizations...\n"]},{"output_type":"stream","name":"stderr","text":["Saving: 100%|██████████| 5247/5247 [01:58<00:00, 44.42it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","============================================================\n","Total Images: 5247\n","Inference + Visualization Time: 227.29 seconds\n","FPS: 23.09 frames per second\n","OOD Detected: 4831 / 5247\n","Output saved to: output_visualizations_final_wo_clahe/\n","============================================================\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]}]}